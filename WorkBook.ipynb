{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string, re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('news.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                           headline  \\\n",
       "0    CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all categories that the articles are categorized by\n",
    "categories = list(df.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CRIME',\n",
       " 'ENTERTAINMENT',\n",
       " 'WORLD NEWS',\n",
       " 'IMPACT',\n",
       " 'POLITICS',\n",
       " 'WEIRD NEWS',\n",
       " 'BLACK VOICES',\n",
       " 'WOMEN',\n",
       " 'COMEDY',\n",
       " 'QUEER VOICES',\n",
       " 'SPORTS',\n",
       " 'BUSINESS',\n",
       " 'TRAVEL',\n",
       " 'MEDIA',\n",
       " 'TECH',\n",
       " 'RELIGION',\n",
       " 'SCIENCE',\n",
       " 'LATINO VOICES',\n",
       " 'EDUCATION',\n",
       " 'COLLEGE',\n",
       " 'PARENTS',\n",
       " 'ARTS & CULTURE',\n",
       " 'STYLE',\n",
       " 'GREEN',\n",
       " 'TASTE',\n",
       " 'HEALTHY LIVING',\n",
       " 'THE WORLDPOST',\n",
       " 'GOOD NEWS',\n",
       " 'WORLDPOST',\n",
       " 'FIFTY',\n",
       " 'ARTS',\n",
       " 'WELLNESS',\n",
       " 'PARENTING',\n",
       " 'HOME & LIVING',\n",
       " 'STYLE & BEAUTY',\n",
       " 'DIVORCE',\n",
       " 'WEDDINGS',\n",
       " 'FOOD & DRINK',\n",
       " 'MONEY',\n",
       " 'ENVIRONMENT',\n",
       " 'CULTURE & ARTS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classfication\n",
    "Going to use the headline and the short description to try and predict the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.headline[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She left her husband. He killed their children. Just another day in America.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.short_description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to use the headline and the short description as the same importance. Combining the two will then be used to try and predict the category that it belongs to based on the words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a period at the end of the headline to be combined\n",
    "#   with the short description\n",
    "df.headline = df.headline+'. '\n",
    "# combining the two together\n",
    "df['text'] = df.headline + df.short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV. She left her husband. He killed their children. Just another day in America.\n"
     ]
    }
   ],
   "source": [
    "print(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.text\n",
    "target = df.category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the more important words in the descriptions I am going to remove the stop words. Stop words are words like and, or, the, and many more that do not give any 'information' on what that article title/short description is describing.\n",
    "\n",
    "If the word 'gun' appears in an article title then the article is most likely about crime and not entertainment. If the word 'baseball' appears in an article then it is most likely about sports. There are interesting words like 'court' where it could be referring to a basketball court or a court where trails in criminal and civil cases are conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to filter out stopwords and punctions to get root words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += list(string.punctuation)\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to remove the stopwords from the data\n",
    "# splitting data into tokens meaning a list of individual words\n",
    "def remove_stopwords(article_text):\n",
    "    '''\n",
    "    INPUT: Text of any kind\n",
    "    OUTPUT: Text with all stop words removed\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(article_text)\n",
    "    article_without_stopwords = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    return article_without_stopwords\n",
    "    \n",
    "data_wo_stopwords = list(map(remove_stopwords,data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal:\n",
      "There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV. She left her husband. He killed their children. Just another day in America.\n",
      "\n",
      "Tokenized + Removed Stop Words:\n",
      "['2', 'mass', 'shootings', 'texas', 'last', 'week', '1', 'tv', 'left', 'husband', 'killed', 'children', 'another', 'day', 'america']\n"
     ]
    }
   ],
   "source": [
    "print('Normal:')\n",
    "print(data[0]+'\\n')\n",
    "print('Tokenized + Removed Stop Words:')\n",
    "print(data_wo_stopwords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118709"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the total vocab is a list of all words used in every article\n",
    "total_vocab = set()\n",
    "for word in data_wo_stopwords:\n",
    "    total_vocab.update(word)\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to lemmatize the text data in order to reduce the total vocab. Lemmatization is the process of removing different conguations of the same word. Organize, organizes, and organizing are all the same word but with different endings to describe if it is happening and when it is happening. The process of lemmatization will remove these so that all 3 words will transform just into organize and will be treated the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_data = []\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for article in data_wo_stopwords:\n",
    "    lemmatized_article = ' '.join([lemmatizer.lemmatize(word) for word in article])\n",
    "    lemmatized_data.append(lemmatized_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal:\n",
      "There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV. She left her husband. He killed their children. Just another day in America. \n",
      "\n",
      "Tokenized + Removed Stop Words:\n",
      "['2', 'mass', 'shootings', 'texas', 'last', 'week', '1', 'tv', 'left', 'husband', 'killed', 'children', 'another', 'day', 'america'] \n",
      "\n",
      "Lemmatized:\n",
      "2 mass shooting texas last week 1 tv left husband killed child another day america\n"
     ]
    }
   ],
   "source": [
    "print('Normal:')\n",
    "print(data[0],'\\n')\n",
    "print('Tokenized + Removed Stop Words:')\n",
    "print(data_wo_stopwords[0],'\\n')\n",
    "print('Lemmatized:')\n",
    "print(lemmatized_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lemmatized_data\n",
    "y = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "# creating the tfidf which will train off the documents bsased on 'important' words\n",
    "tfidf_train = tfidf.fit_transform(X_train)\n",
    "tfidf_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<160682x71956 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2591387 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that this is an extremely sparse matrix meaning there are many 0 values meaning that words are only in a few amount of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Articles: 16.127425598386875\n",
      "Percentage of columns containing ZERO: 0.9997758710100841\n"
     ]
    }
   ],
   "source": [
    "non_zero_cols = tfidf_train.nnz / float(tfidf_train.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_train.shape[1]))\n",
    "print('Percentage of columns containing ZERO: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to look at what words are associated with each category. These are the words that are most frequent within the documents of each category. Just for example going to do 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolating categories\n",
    "df_crime = df[df.category == 'CRIME']\n",
    "df_entertainment = df[df.category == 'ENTERTAINMENT']\n",
    "df_sports = df[df.category == 'SPORTS']\n",
    "df_tech = df[df.category == 'TECH']\n",
    "df_politics = df[df.category == 'POLITICS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolating text from the articles\n",
    "crime_data = df_crime.text\n",
    "entertainment_data = df_entertainment.text\n",
    "sports_data = df_sports.text\n",
    "tech_data = df_tech.text\n",
    "politics_data = df_politics.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the the same removing of stop words that we did before\n",
    "crime_clean = list(map(remove_stopwords,crime_data))\n",
    "entertainment_clean = list(map(remove_stopwords,entertainment_data))\n",
    "sports_clean = list(map(remove_stopwords,sports_data))\n",
    "tech_clean = list(map(remove_stopwords,tech_data))\n",
    "politics_clean = list(map(remove_stopwords,politics_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is now in a list of lists and to look at words \n",
    "#. frequency we need a flat list of n x 1\n",
    "crime_flat = [item for sublist in crime_clean for item in sublist]\n",
    "entertainment_flat = [item for sublist in entertainment_clean for item in sublist]\n",
    "sports_flat = [item for sublist in sports_clean for item in sublist]\n",
    "tech_flat = [item for sublist in tech_clean for item in sublist]\n",
    "politics_flat = [item for sublist in politics_clean for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will use frequency distribution to identify the w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
